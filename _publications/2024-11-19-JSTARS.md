---
title: "Human-annotated label noise and their impact on ConvNets for remote sensing image scene classification"
collection: publications
permalink: /publication/2024-11-19-JSTARS
excerpt: 'This paper explores the impact of label noise on deep learning.'
date: 2024-11-19
venue: 'IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing'
paperurl: # 
citation: 'Peng, L., Wei, T., Chen, X., Chen, X., Sun, R., Wan, L., ... & Zhu, X. (2024). Human-annotated label noise and their impact on ConvNets for remote sensing image scene classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing.'
---

Human-labeled training datasets are essential for convolutional neural networks (ConvNets) in satellite image scene classification. Annotation errors are unavoidable due to the complexity of satellite images. However, the distribution of real-world human-annotated label noises on satellite images and their impact on ConvNets have not been investigated. To fill this research gap, this article, for the first time, collected real-world labels from 32 participants and explored how their annotated label noise affects three representative ConvNets (VGG16, GoogleNet, and ResNet-50) for remote sensing image scene classification. We found that 1) human-annotated label noise exhibits significant class and instance dependence; 2) an additional 1% of human-annotated label noise in training data leads to a 0.5% reduction in the overall accuracy of ConvNets classification; and 3) the error pattern of ConvNet predictions was strongly correlated with that of participant's labels. To uncover the mechanism underlying the impact of human labeling errors on ConvNets, we compared it with three types of simulated label noise: uniform noise, class-dependent noise, and instance-dependent noise. Our results show that the impact of human-annotated label noise on ConvNets significantly differs from all three types of simulated label noise, while both class dependence and instance dependence contribute to the impact of human-annotated label noise on ConvNets. Additionally, the label noise estimation algorithm (confident learning) cannot fully identify label noise. These observations necessitate a reevaluation of the handling of noisy labels, and we anticipate that our real-world label noise dataset would facilitate the future development and assessment of label-noise learning algorithms.
